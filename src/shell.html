<!doctype html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>C++ Audio (DSP Worker)</title>
    <style>
        body { font-family: sans-serif; text-align: center; }
        #controlButton { font-size: 1.5em; }
    </style>
  </head>
  <body>
    <h1>C++ WebAssembly Audio Engine (With a dedicated DSP Worker)</h1>
    <button id="controlButton">Start Audio</button>
    <div id="status"></div>
    <script src="coi-serviceworker.min.js"></script>

    <!-- This special tag is where Emscripten injects its JS loader -->
    {{{ SCRIPT }}}

    <script type='text/javascript'>
      const controlButton = document.getElementById('controlButton');
      const statusElement = document.getElementById('status');

      // This is the global "struct" to hold all our audio components.
      const audioSystem = {
        audioContext: null,
        dspWorker: null,
      };


      // This self-executing async function is our main entry point.
      (async () => {
        if (typeof SharedArrayBuffer === 'undefined') {
          statusElement.innerHTML = "SharedArrayBuffer is not available. <br>Try reloading the page"
          controlButton.disabled = true;
          return; // Stop execution; the page will reload.
        }

        // If we get here, SharedArrayBuffer is available. It's safe to proceed.
        statusElement.textContent = 'COOP/COEP Headers active. Waiting for WASM runtime...';

        // Step 2: Set up the Emscripten Module object in the global scope.
        var Module = {
          onRuntimeInitialized: () => {
            console.log('Emscripten runtime initialized.');
            Module.callMain();
            controlButton.disabled = false;
            controlButton.addEventListener('click', toggleAudio);
            statusElement.textContent = 'WASM Ready. Click Start Audio to initialize audio systems.';
          }
        };
      })();

      // This function is now the single, atomic setup-and-play entry point.
      async function setupAudio() {
        statusElement.textContent = `Initializing audio systems...`;

        audioSystem.audioContext = new (window.AudioContext || window.webkitAudioContext)();
        audioSystem.dspWorker = new Worker('dsp-worker.js');
        await audioSystem.audioContext.audioWorklet.addModule('audioworklet-processor.js');

        const buffer_frames = 8192;
        const audio_sab = new SharedArrayBuffer(buffer_frames * 2 * 4);
        const state_sab = new SharedArrayBuffer(2 * 4);

        // Get the real Sample Rate
        const actual_sample_rate = audioSystem.audioContext.sampleRate;
        statusElement.textContent = `AudioContext created at ${actual_sample_rate} Hz. Waiting for DSP worker...`;

        // We now wrap the entire DSP worker initialization in a Promise.
        // This function will not resolve until the worker sends the 'ready' message.
        await new Promise(resolve => {
            audioSystem.dspWorker.onmessage = (event) => {
                if (event.data.type === 'ready') {
                    console.log("DSP Worker is ready.");
                    resolve(); // Resolve the promise, allowing setupAudio to continue.
                }
            };
            
            // Send the initialization message to the DSP worker.
            audioSystem.dspWorker.postMessage({
                type: 'init',
                wasm_module_name: 'CrossPlatformAudio.js',
                sample_rate: actual_sample_rate,
                audio_sab: audio_sab,
                state_sab: state_sab
            });
        });

        statusElement.textContent = 'All systems ready.';
        const workletNode = new AudioWorkletNode(audioSystem.audioContext, 'cpp-audio-processor');
        workletNode.port.postMessage({ audio_sab: audio_sab, state_sab: state_sab });

        // The handler for when the Audio Worklet requests more data.
        workletNode.port.onmessage = (event) => {
          if (event.data.type === 'request_data') {
            audioSystem.dspWorker.postMessage({ type: 'request_data' });
          }
        };

        workletNode.connect(audioSystem.audioContext.destination);
      }

      async function toggleAudio() {
        // On the first click, the context is null. We run the full, atomic setup.
        if (!audioSystem.audioContext) {
            try {
                await setupAudio();
            } catch (err) {
                statusElement.textContent = `Error during setup: ${err.message}`;
                console.error(err);
                return;
            }
        }

        // Now we are guaranteed that the system is fully initialized.
        if (audioSystem.audioContext.state === 'running') {
            await audioSystem.audioContext.suspend();
            controlButton.textContent = 'Resume Audio';
            statusElement.textContent = 'Status: Paused.';
        } else {
            await audioSystem.audioContext.resume();
            controlButton.textContent = 'Pause Audio';
            statusElement.textContent = 'Status: Playing.';
        }
      }

    </script>
  </body>
</html>

